{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required modules\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F \n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from collections import OrderedDict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying transformations on images\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
    "\n",
    "#Loading training and testing set\n",
    "trainset = datasets.MNIST('MNIST_data/', train=True, download=True, transform=transform)\n",
    "testset = datasets.MNIST('MNIST_data/', train=False, download=True, transform=transform)\n",
    "\n",
    "#size of validation dataset\n",
    "valid_size = 0.2\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(trainset)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, sampler=train_sampler)\n",
    "validloader = torch.utils.data.DataLoader(trainset, batch_size=64, sampler=valid_sampler)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (dropout1): Dropout(p=0.2)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (dropout2): Dropout(p=0.2)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (log_soft1): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining model\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('fc1', nn.Linear(784, 128)),\n",
    "    ('relu1', nn.ReLU()),\n",
    "    ('dropout1', nn.Dropout(0.2)),\n",
    "    ('fc2', nn.Linear(128, 64)),\n",
    "    ('relu2', nn.ReLU()),\n",
    "    ('dropout2', nn.Dropout(0.2)),\n",
    "    ('fc3', nn.Linear(64, 10)),\n",
    "    ('log_soft1', nn.LogSoftmax(dim=1))\n",
    "]))\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the criterion as Negative Log Likelihood Loss\n",
    "criterion = nn.NLLLoss()\n",
    "# setting the optimizer to Stocastic Gradient Descent\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1/40 .. Training Loss: 2.074 .. Valid Loss: 1.665 .. Accuracy: 0.572\n",
      "Validation loss decreased (inf)->(1.665) .. saving model\n",
      "Epochs: 2/40 .. Training Loss: 1.358 .. Valid Loss: 0.959 .. Accuracy: 0.751\n",
      "Validation loss decreased (1.665)->(0.959) .. saving model\n",
      "Epochs: 3/40 .. Training Loss: 0.948 .. Valid Loss: 0.695 .. Accuracy: 0.814\n",
      "Validation loss decreased (0.959)->(0.695) .. saving model\n",
      "Epochs: 4/40 .. Training Loss: 0.761 .. Valid Loss: 0.569 .. Accuracy: 0.849\n",
      "Validation loss decreased (0.695)->(0.569) .. saving model\n",
      "Epochs: 5/40 .. Training Loss: 0.647 .. Valid Loss: 0.490 .. Accuracy: 0.867\n",
      "Validation loss decreased (0.569)->(0.490) .. saving model\n",
      "Epochs: 6/40 .. Training Loss: 0.580 .. Valid Loss: 0.442 .. Accuracy: 0.879\n",
      "Validation loss decreased (0.490)->(0.442) .. saving model\n",
      "Epochs: 7/40 .. Training Loss: 0.531 .. Valid Loss: 0.409 .. Accuracy: 0.889\n",
      "Validation loss decreased (0.442)->(0.409) .. saving model\n",
      "Epochs: 8/40 .. Training Loss: 0.496 .. Valid Loss: 0.381 .. Accuracy: 0.891\n",
      "Validation loss decreased (0.409)->(0.381) .. saving model\n",
      "Epochs: 9/40 .. Training Loss: 0.466 .. Valid Loss: 0.358 .. Accuracy: 0.899\n",
      "Validation loss decreased (0.381)->(0.358) .. saving model\n",
      "Epochs: 10/40 .. Training Loss: 0.442 .. Valid Loss: 0.344 .. Accuracy: 0.899\n",
      "Validation loss decreased (0.358)->(0.344) .. saving model\n",
      "Epochs: 11/40 .. Training Loss: 0.419 .. Valid Loss: 0.326 .. Accuracy: 0.905\n",
      "Validation loss decreased (0.344)->(0.326) .. saving model\n",
      "Epochs: 12/40 .. Training Loss: 0.406 .. Valid Loss: 0.315 .. Accuracy: 0.907\n",
      "Validation loss decreased (0.326)->(0.315) .. saving model\n",
      "Epochs: 13/40 .. Training Loss: 0.388 .. Valid Loss: 0.304 .. Accuracy: 0.910\n",
      "Validation loss decreased (0.315)->(0.304) .. saving model\n",
      "Epochs: 14/40 .. Training Loss: 0.376 .. Valid Loss: 0.298 .. Accuracy: 0.912\n",
      "Validation loss decreased (0.304)->(0.298) .. saving model\n",
      "Epochs: 15/40 .. Training Loss: 0.367 .. Valid Loss: 0.287 .. Accuracy: 0.915\n",
      "Validation loss decreased (0.298)->(0.287) .. saving model\n",
      "Epochs: 16/40 .. Training Loss: 0.354 .. Valid Loss: 0.278 .. Accuracy: 0.918\n",
      "Validation loss decreased (0.287)->(0.278) .. saving model\n",
      "Epochs: 17/40 .. Training Loss: 0.345 .. Valid Loss: 0.271 .. Accuracy: 0.919\n",
      "Validation loss decreased (0.278)->(0.271) .. saving model\n",
      "Epochs: 18/40 .. Training Loss: 0.333 .. Valid Loss: 0.266 .. Accuracy: 0.920\n",
      "Validation loss decreased (0.271)->(0.266) .. saving model\n",
      "Epochs: 19/40 .. Training Loss: 0.323 .. Valid Loss: 0.258 .. Accuracy: 0.922\n",
      "Validation loss decreased (0.266)->(0.258) .. saving model\n",
      "Epochs: 20/40 .. Training Loss: 0.316 .. Valid Loss: 0.253 .. Accuracy: 0.925\n",
      "Validation loss decreased (0.258)->(0.253) .. saving model\n",
      "Epochs: 21/40 .. Training Loss: 0.309 .. Valid Loss: 0.246 .. Accuracy: 0.926\n",
      "Validation loss decreased (0.253)->(0.246) .. saving model\n",
      "Epochs: 22/40 .. Training Loss: 0.300 .. Valid Loss: 0.241 .. Accuracy: 0.928\n",
      "Validation loss decreased (0.246)->(0.241) .. saving model\n",
      "Epochs: 23/40 .. Training Loss: 0.296 .. Valid Loss: 0.235 .. Accuracy: 0.930\n",
      "Validation loss decreased (0.241)->(0.235) .. saving model\n",
      "Epochs: 24/40 .. Training Loss: 0.288 .. Valid Loss: 0.229 .. Accuracy: 0.931\n",
      "Validation loss decreased (0.235)->(0.229) .. saving model\n",
      "Epochs: 25/40 .. Training Loss: 0.281 .. Valid Loss: 0.224 .. Accuracy: 0.932\n",
      "Validation loss decreased (0.229)->(0.224) .. saving model\n",
      "Epochs: 26/40 .. Training Loss: 0.274 .. Valid Loss: 0.220 .. Accuracy: 0.934\n",
      "Validation loss decreased (0.224)->(0.220) .. saving model\n",
      "Epochs: 27/40 .. Training Loss: 0.270 .. Valid Loss: 0.216 .. Accuracy: 0.934\n",
      "Validation loss decreased (0.220)->(0.216) .. saving model\n",
      "Epochs: 28/40 .. Training Loss: 0.265 .. Valid Loss: 0.212 .. Accuracy: 0.936\n",
      "Validation loss decreased (0.216)->(0.212) .. saving model\n",
      "Epochs: 29/40 .. Training Loss: 0.259 .. Valid Loss: 0.209 .. Accuracy: 0.936\n",
      "Validation loss decreased (0.212)->(0.209) .. saving model\n",
      "Epochs: 30/40 .. Training Loss: 0.256 .. Valid Loss: 0.204 .. Accuracy: 0.939\n",
      "Validation loss decreased (0.209)->(0.204) .. saving model\n",
      "Epochs: 31/40 .. Training Loss: 0.250 .. Valid Loss: 0.200 .. Accuracy: 0.940\n",
      "Validation loss decreased (0.204)->(0.200) .. saving model\n",
      "Epochs: 32/40 .. Training Loss: 0.247 .. Valid Loss: 0.197 .. Accuracy: 0.941\n",
      "Validation loss decreased (0.200)->(0.197) .. saving model\n",
      "Epochs: 33/40 .. Training Loss: 0.241 .. Valid Loss: 0.194 .. Accuracy: 0.942\n",
      "Validation loss decreased (0.197)->(0.194) .. saving model\n",
      "Epochs: 34/40 .. Training Loss: 0.237 .. Valid Loss: 0.190 .. Accuracy: 0.943\n",
      "Validation loss decreased (0.194)->(0.190) .. saving model\n",
      "Epochs: 35/40 .. Training Loss: 0.233 .. Valid Loss: 0.190 .. Accuracy: 0.943\n",
      "Validation loss decreased (0.190)->(0.190) .. saving model\n",
      "Epochs: 36/40 .. Training Loss: 0.228 .. Valid Loss: 0.184 .. Accuracy: 0.945\n",
      "Validation loss decreased (0.190)->(0.184) .. saving model\n",
      "Epochs: 37/40 .. Training Loss: 0.226 .. Valid Loss: 0.182 .. Accuracy: 0.945\n",
      "Validation loss decreased (0.184)->(0.182) .. saving model\n",
      "Epochs: 38/40 .. Training Loss: 0.222 .. Valid Loss: 0.179 .. Accuracy: 0.946\n",
      "Validation loss decreased (0.182)->(0.179) .. saving model\n",
      "Epochs: 39/40 .. Training Loss: 0.217 .. Valid Loss: 0.176 .. Accuracy: 0.947\n",
      "Validation loss decreased (0.179)->(0.176) .. saving model\n",
      "Epochs: 40/40 .. Training Loss: 0.214 .. Valid Loss: 0.172 .. Accuracy: 0.949\n",
      "Validation loss decreased (0.176)->(0.172) .. saving model\n"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "running_loss = 0\n",
    "min_valid_loss = np.Inf\n",
    "\n",
    "steps = 0\n",
    "print_every = 500\n",
    "\n",
    "for e in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    for images, labels in trainloader:\n",
    "        steps += 1\n",
    "        \n",
    "        # This step will change the shape of images from (64,1,28,28) to (64, 784)\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # feed-forward\n",
    "        logits = model(images)\n",
    "        \n",
    "        #calculating the loss\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Doing backward propagation of loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    \n",
    "    #validation\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    accuracy = 0\n",
    "    for images, labels in validloader:\n",
    "        images = images.view(images.shape[0], -1)\n",
    "\n",
    "        logps = model(images)\n",
    "        valid_loss += criterion(logps, labels).item()\n",
    "        ps = torch.exp(logps)\n",
    "\n",
    "        out_class, out_p = ps.topk(1)\n",
    "        equality = labels==out_p.view(*labels.shape)\n",
    "        accuracy += torch.mean(equality.type(torch.FloatTensor))\n",
    "        \n",
    "\n",
    "            \n",
    "    print('Epochs: {}/{} .. Training Loss: {:.3f} .. Valid Loss: {:.3f} .. Accuracy: {:.3f}'.format(e+1, \\\n",
    "                                                                                                     epochs, \\\n",
    "                                                                                                     running_loss/len(trainloader), \\\n",
    "                                                                                                     valid_loss/len(validloader), \\\n",
    "                                                                                                     accuracy/len(validloader)))\n",
    "    if valid_loss <= min_valid_loss:\n",
    "        print('Validation loss decreased ({:.3f})->({:.3f}) .. saving model'.format(min_valid_loss/len(validloader), \\\n",
    "                                                                                    valid_loss/len(validloader)))\n",
    "        torch.save(model.state_dict(), 'checkpoint.pth')\n",
    "        min_valid_loss = valid_loss\n",
    "    running_loss=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our validation and training loss still decreases even on the 40th epoch. We could even train the model for more number of epochs and expect good accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the saved model\n",
    "model.load_state_dict(torch.load('checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on testing data: 0.159\n",
      "Accuracy on testing data:0.953\n"
     ]
    }
   ],
   "source": [
    "#testing the model on test data\n",
    "\n",
    "testing_loss = 0\n",
    "accuracy = 0\n",
    "\n",
    "for images, labels in testloader:\n",
    "    images = images.view(images.shape[0], -1)\n",
    "    \n",
    "    logps = model(images)\n",
    "    loss = criterion(logps, labels)\n",
    "    ps = torch.exp(logps)\n",
    "    \n",
    "    out_class, out_p = ps.topk(1)\n",
    "    equality = labels==out_p.view(*labels.shape)\n",
    "    accuracy += torch.mean(equality.type(torch.FloatTensor))\n",
    "    testing_loss +=loss\n",
    "\n",
    "print('Loss on testing data: {:.3f}\\nAccuracy on testing data:{:.3f}'.format(testing_loss/len(testloader), accuracy/len(testloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x26b8b4f1b70>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(testloader))\n",
    "\n",
    "# changing the shape of images[0] from (1,28,28) to (1,784)\n",
    "img = images[0].view(1,-1)\n",
    "\n",
    "# Defined a figure of size(14,6)\n",
    "plt.figure(figsize=[14,6])\n",
    "\n",
    "# forward passing the 'img'\n",
    "logits = model(img)\n",
    "# using torch.exp() gives out model gives outputs as los_softmax and doing exp() we will get original values.\n",
    "output = torch.exp(logits)\n",
    "\n",
    "# converts 'output' tensor to numpy array\n",
    "y = output.detach().numpy()\n",
    "N = y.shape[1]\n",
    "x = np.arange(N)\n",
    "\n",
    "# Sub plot 1st of plots with 1 row and 2 columns\n",
    "plt.subplot(1,2,1)\n",
    "plt.bar(x,y.ravel())\n",
    "\n",
    "\n",
    "# Sub plot 2nd of plots with 1 row and 2 columns\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(images[0].numpy().squeeze(), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
